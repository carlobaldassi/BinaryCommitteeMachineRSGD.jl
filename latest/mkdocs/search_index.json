{
    "docs": [
        {
            "location": "/", 
            "text": "BinaryCommitteeMachineRSGD.jl documentation\n\n\nThis package implements the Replicated Stochastic Gradient Descent algorithm for committee machines with binary weights described in the paper \nUnreasonable Effectiveness of Learning Neural Networks: From Accessible States and Robust Ensembles to Basic Algorithmic Schemes\n by Carlo Baldassi, Christian Borgs, Jennifer Chayes, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti and Riccardo Zecchina, Proc. Natl. Acad. Sci. U.S.A. (2016), \ndoi:10.1073/pnas.1608103113\n.\n\n\nThe package is tested against Julia \n0.4\n, \n0.5\n and \ncurrent\n \n0.6-dev\n on Linux, OS X, and Windows.\n\n\n\n\nInstallation\n\n\nTo install the module, use this command from within Julia:\n\n\njulia\n Pkg.clone(\nhttps://github.com/carlobaldassi/BinaryCommitteeMachineRSGD.jl\n)\n\n\n\n\nDependencies will be installed automatically.\n\n\n\n\nUsage\n\n\nThe module is loaded as any other Julia module:\n\n\njulia\n using BinaryCommitteeMachineRSGD\n\n\n\n\nThe code basically provides a single function which generates a system of interacting replicated committee machines and tries to learn some patterns. The function and the patterns constructor are documented below.\n\n\n#\n\n\nBinaryCommitteeMachineRSGD.Patterns\n \n \nType\n.\n\n\nPatterns(N, M)\n\n\n\n\nGenerates \nM\n random \u00b11 patterns of length \nN\n.\n\n\nPatterns(\u03be, \u03c3)\n\n\n\n\nEncapsulates the input patterns \n\u03be\n and their associated desired outputs \n\u03c3\n for use in \nreplicatedSGD\n. The inputs \n\u03be\n must be given as a vector of vectors, while the outputs \n\u03c3\n must be given as a vector. In both cases, they are converted to \u00b11 values using their sign (more precisely, using \nx \n 0 ? 1 : -1\n).\n\n\nsource\n\n\n#\n\n\nBinaryCommitteeMachineRSGD.replicatedSGD\n \n \nFunction\n.\n\n\nreplicatedSGD(patterns::Patterns; keywords...)\n\n\n\n\nRuns the replicated Stochastic Gradient Descent algorithm over the given \npatterns\n (see \nPatterns\n). It automatically detects the size of the input and initializes a system of interacting binary committee machines which collectively try to learn the patterns.\n\n\nThe function returns three values: a \nBool\n with the success status, the number of epochs, and the minimum error achieved.\n\n\nThe available keyword arguments (note that the defaults are mostly \nnot\n sensible, they must be collectively tuned):\n\n\n\n\nK\n (default=\n1\n): number of hidden units for each committee machine (size of the hidden layer)\n\n\ny\n (default=\n1\n): number of replicas\n\n\n\u03b7\n (default=\n2\n): initial value of the step for the energy (loss) term gradient\n\n\n\u03bb\n (default=\n0.1\n): initial value of the step for the interaction gradient (called \n\u03b7\u2032\n in the paper)\n\n\n\u03b3\n (default=\nInf\n): initial value of the interaction strength\n\n\n\u03b7factor\n (default=\n1\n): factor used to update \n\u03b7\n after each epoch\n\n\n\u03bbfactor\n (default=\n1\n): factor used to update \n\u03bb\n after each epoch\n\n\n\u03b3step\n (default=\n0.01\n): additive step used to update \n\u03b3\n after each epoch\n\n\nbatch\n (default=\n5\n): minibatch size\n\n\nformula\n (default=\n:simple\n): used to choose the interaction update scheme when \ncenter=false\n; see below for available values\n\n\nseed\n (default=\n0\n): random seed; if \n0\n, it is not used\n\n\nmax_epochs\n (default=\n1000\n): maximum number of epochs\n\n\ninit_equal\n (default=\ntrue\n): whether to initialize all replicated networks equally\n\n\nwaitcenter\n (default=\nfalse\n): whether to only exit successfully if the center replica has solved the problem\n\n\ncenter\n (default=\nfalse\n): whether to explicity use a central replica (if \nfalse\n, it is traced out)\n\n\noutfile\n (default=\n\"\"\n): name of a file where to output the results; if empty it's ignored\n\n\nquiet\n (default=\nfalse\n): whether to output information on screen\n\n\n\n\nThe possible values of the \nformula\n option are:\n\n\n\n\n:simple\n (the default): uses the simplest traced-out center formula (eq. (C7) in the paper)\n\n\n:corrected\n: applies the correction of eq. (C9) to the formula of eq. (C7)\n\n\n:continuous\n: version in which the center is continuous and traced-out\n\n\n:hard\n: same as \n:simple\n but uses a hard tanh, for improved performance\n\n\n\n\nExample of a good parameter configuration (for a committee with \nK=5\n and \nN*K=1605\n synapses overall, working at \n\u03b1=M/(NK)=0.5\n):\n\n\nok, epochs, minerr = replicatedSGD(Patterns(321, 802), K=5, y=7, batch=80, \u03bb=0.75, \u03b3=0.05, \u03b3step=0.001, formula=:simple)\n\n\n\n\nsource", 
            "title": "Home"
        }, 
        {
            "location": "/#binarycommitteemachinersgdjl-documentation", 
            "text": "This package implements the Replicated Stochastic Gradient Descent algorithm for committee machines with binary weights described in the paper  Unreasonable Effectiveness of Learning Neural Networks: From Accessible States and Robust Ensembles to Basic Algorithmic Schemes  by Carlo Baldassi, Christian Borgs, Jennifer Chayes, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti and Riccardo Zecchina, Proc. Natl. Acad. Sci. U.S.A. (2016),  doi:10.1073/pnas.1608103113 .  The package is tested against Julia  0.4 ,  0.5  and  current   0.6-dev  on Linux, OS X, and Windows.", 
            "title": "BinaryCommitteeMachineRSGD.jl documentation"
        }, 
        {
            "location": "/#installation", 
            "text": "To install the module, use this command from within Julia:  julia  Pkg.clone( https://github.com/carlobaldassi/BinaryCommitteeMachineRSGD.jl )  Dependencies will be installed automatically.", 
            "title": "Installation"
        }, 
        {
            "location": "/#usage", 
            "text": "The module is loaded as any other Julia module:  julia  using BinaryCommitteeMachineRSGD  The code basically provides a single function which generates a system of interacting replicated committee machines and tries to learn some patterns. The function and the patterns constructor are documented below.  #  BinaryCommitteeMachineRSGD.Patterns     Type .  Patterns(N, M)  Generates  M  random \u00b11 patterns of length  N .  Patterns(\u03be, \u03c3)  Encapsulates the input patterns  \u03be  and their associated desired outputs  \u03c3  for use in  replicatedSGD . The inputs  \u03be  must be given as a vector of vectors, while the outputs  \u03c3  must be given as a vector. In both cases, they are converted to \u00b11 values using their sign (more precisely, using  x   0 ? 1 : -1 ).  source  #  BinaryCommitteeMachineRSGD.replicatedSGD     Function .  replicatedSGD(patterns::Patterns; keywords...)  Runs the replicated Stochastic Gradient Descent algorithm over the given  patterns  (see  Patterns ). It automatically detects the size of the input and initializes a system of interacting binary committee machines which collectively try to learn the patterns.  The function returns three values: a  Bool  with the success status, the number of epochs, and the minimum error achieved.  The available keyword arguments (note that the defaults are mostly  not  sensible, they must be collectively tuned):   K  (default= 1 ): number of hidden units for each committee machine (size of the hidden layer)  y  (default= 1 ): number of replicas  \u03b7  (default= 2 ): initial value of the step for the energy (loss) term gradient  \u03bb  (default= 0.1 ): initial value of the step for the interaction gradient (called  \u03b7\u2032  in the paper)  \u03b3  (default= Inf ): initial value of the interaction strength  \u03b7factor  (default= 1 ): factor used to update  \u03b7  after each epoch  \u03bbfactor  (default= 1 ): factor used to update  \u03bb  after each epoch  \u03b3step  (default= 0.01 ): additive step used to update  \u03b3  after each epoch  batch  (default= 5 ): minibatch size  formula  (default= :simple ): used to choose the interaction update scheme when  center=false ; see below for available values  seed  (default= 0 ): random seed; if  0 , it is not used  max_epochs  (default= 1000 ): maximum number of epochs  init_equal  (default= true ): whether to initialize all replicated networks equally  waitcenter  (default= false ): whether to only exit successfully if the center replica has solved the problem  center  (default= false ): whether to explicity use a central replica (if  false , it is traced out)  outfile  (default= \"\" ): name of a file where to output the results; if empty it's ignored  quiet  (default= false ): whether to output information on screen   The possible values of the  formula  option are:   :simple  (the default): uses the simplest traced-out center formula (eq. (C7) in the paper)  :corrected : applies the correction of eq. (C9) to the formula of eq. (C7)  :continuous : version in which the center is continuous and traced-out  :hard : same as  :simple  but uses a hard tanh, for improved performance   Example of a good parameter configuration (for a committee with  K=5  and  N*K=1605  synapses overall, working at  \u03b1=M/(NK)=0.5 ):  ok, epochs, minerr = replicatedSGD(Patterns(321, 802), K=5, y=7, batch=80, \u03bb=0.75, \u03b3=0.05, \u03b3step=0.001, formula=:simple)  source", 
            "title": "Usage"
        }
    ]
}